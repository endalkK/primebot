# -*- coding: utf-8 -*-
"""primeBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N1dbMlxr8c-8VQez-ueuwemX0ggrnBdS
"""
from datasets import load_dataset

# Load the dataset
dataset = load_dataset('json', data_files='/content/drive/MyDrive/PrimeBot/dataset.json')

# Split the dataset into training and validation sets
dataset = dataset['train'].train_test_split(test_size=0.1)  # 10% for validation

# Print out dataset info
print(dataset)

from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM

# Load pre-trained model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# Load and preprocess the dataset
def preprocess_function(examples):
    conversations = []
    for ex in examples['conversation']:
        if isinstance(ex, list):
            conversations.append(" ".join([item.get('text', '') if isinstance(item, dict) else str(item) for item in ex]))
        elif isinstance(ex, dict):
            conversations.append(ex.get('text', ''))
        else:
            conversations.append(str(ex))
    inputs = tokenizer(conversations, padding=True, truncation=True, return_tensors="pt")
    inputs['labels'] = inputs['input_ids'].clone()
    return inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    output_dir="./results",
    num_train_epochs=5,  # Increase number of epochs
    learning_rate=5e-5,  # Lower learning rate
    logging_dir="./logs",
    logging_steps=10,
)


# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'], # Changed 'validation' to 'test'
)

# Fine-tune the model
trainer.train()

# Save the model
model.save_pretrained("./fine-tuned-dialoGPT")
tokenizer.save_pretrained("./fine-tuned-dialoGPT")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the fine-tuned model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("./fine-tuned-dialoGPT")
model = AutoModelForCausalLM.from_pretrained("./fine-tuned-dialoGPT")

# Example: Generating a response
def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs.get('attention_mask'),
        max_length=150,
        temperature=0.7,  # Lowered for less randomness
        top_k=40,         # Reducing randomness
        top_p=0.9,        # Increase diversity but reduce randomness
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# Test the model
prompt = "Tell me a joke."
response = generate_response(prompt)
print(f"Prompt: {prompt}")
print(f"Response: {response}")
